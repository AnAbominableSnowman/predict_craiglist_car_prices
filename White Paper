## Marketing Tracking of Craiglist's Used Cars

## Table of Contents
1. [Introduction](#introduction)
2. [Problem Definition](#problem-definition)
3. [Data Collection](#data-collection)
4. [Exploratory Data Analysis (EDA)](#exploratory-data-analysis-eda)
5. [Data Preprocessing](#data-preprocessing)
6. [Feature Engineering](#feature-engineering)
7. [Modeling](#modeling)
8. [Model Evaluation](#model-evaluation)
9. [Conclusion and Future Work](#conclusion-and-future-work)
10. [References](#references)

## Introduction
**Growing up, I had a family friend who was always poring over Craigslist ads, buying cars, fixing them up, and flipping them for profit. Each flip took him at least forty hours of searching. I can still picture him, bathed in the dim glow of a cathode ray tube, surrounded by stacks of booklets.**

This project is an ode to that family friend. It aims to build a tool to predict Craigslist car prices—similar to the tools used by Kelley Blue Book and captive financial companies like Ford Financing. Our anonymous hero could use this tool to efficiently find underpriced offers and snatch them up before others do.

Taking it a step further, our hero could even create a synthetic dataset of a thousand representative vehicles, train the model month by month, and observe how the prices of your synthetic vehicles change over time. Knowing the test data was fixed, any changes in price are attributable to changes in the trained model. This could be used as an index providing valuable insights into market trends and movements.


## Problem Definition

A generous contributor has scraped 450,000 car postings from Craigslist and shared them on [Kaggle](https://www.kaggle.com/datasets/austinreese/craigslist-carstrucks-data/code). I have downloaded the zipped CSV file, and this is where our analysis begins. The target variable is the price, represented in USD. The dataset includes eighteen useful covariates, such as a region, a metro area, price in USD, year, year of manufacture, manufacturer (e.g., Ram or Jeep), model (e.g., Silverado or Forester), condition (e.g., good or fair), cylinders (e.g., 4 or 6), fuel (e.g., diesel or gas), odometer (miles traveled), title status (e.g., rebuilt or clean), transmission (e.g., automatic or manual), drive (e.g., FWD or RWD), type (e.g., sedan or truck), paint color (e.g., white or black), state (e.g., Wisconsin), and lat(itude) and long(itude) (representing geographical location).

## Exploratory Data Analysis (EDA)

### 1. Data Overview
For a thorough EDA using `y_data_profiling`, see the visualizations from [pre_proccessing](results/data_profile_cleaned_subsampled_to_one_percent) and [post processing](results/data_profile_raw_subsampled_to_one_percent). In the interest of space, I will mention the most pertinent details. The price variable is heavily right-skewed, as is the odometer reading, which contains some incredible values of 10 million or more—equivalent to twenty round trips to the moon. It’s safe to say there are some data quality issues, which we will address in [data preprocessing](#data-preprocessing). Another pertinent topic is the high frequency of Carvana ads at XYZ%; since they exhibit higher data quality and more consistency across ads, they form an interesting part of the puzzle. However, their descriptions are mostly identicial boilerplate across many ads. 


Digging into correlations below, we see a three-way interaction among price, age, and odometer reading. Older vehicles typically have more miles and are worth less; untangling mileage from age is a complex problem that is be beyond the scope of this paper. We also notice interactions between (word frequnecy)[#feature-engineering] and odometer readings, suggesting that certain words are used more or less frequently depending on car mileage; this aligns with expectations. No one describes their brand new sports car as "reliable"; that term of endearment is typically reserved for the family workhorse van that has been picking up groceries for a decade or more. Note, for brevity, tf_idf words and correlations aren't in the paper but can be found [here](results)

Finally, the manufacturer shows a correlation with both the number of cylinders and transmission type, which again conforms to common sense. Additionally, about eleven percent of the rows appear to be exact duplicates. While they could represent truly different cars, the exact matching of price, odometer, color, etc., strains credibility, so we will drop these results. Likely people are reposting the same ad to make their ad appear new. 
<div style="display: flex; gap: 40px;">
    <img src="results/Simple Linear Regression of Price by Odometer/corrgram_clean.png" alt="Image 1">
    <!-- <img src="results/Simple Linear Regression of Price by Odometer/ols_results.png" alt="Image 2"> -->
</div>

### 2. Data Visualization
- Histograms of numerical features (e.g., price distribution)
- Scatter plots (e.g., price vs. mileage, price vs. year)
- Correlation heatmaps to show relationships between features

## Data Preprocessing and Feature Engineering
### 1. Handling Missing Values
Missing price or odometer readings were removed, as they are too crucial to attempt imputation. Missing values were only imputed for the linear regression approach, using a standard mean or mode method. A brief examination of missing values is in order. All columns likely exhibit a mix of Missing Not at Random (MNAR) and Missing at Random (MAR). Missing descriptions are probably absent because it’s difficult to provide a good description for a poorly maintained car. Conversely, a rusted, sun-beaten car missing a paint color is missing at random, as the absence is tied to the condition of the car rather than the paint color itself. This type of missingness should cast some doubt on any results derived from the linear regression analysis. Being more frank, any analysis should be somewhat questioned with data accidentally, intentionally and perhaps maliciously missing. 

### 2 Filtering Data
Craigslist seems especially prone to messy data for two reasons: individual posters are not professionals with a corporate image, and hiding information is generally advantageous—most importantly, due to price anchoring. Good cars that are worth a lot are typically owned by more internet-savvy individuals, who understand that spending time to create a great ad to gain even a 2% increase in price is worth it for a more expensive car. Conversely, the negative aspects of a car are often best left unmentioned and discussed later if inquiries arise.

The other key problem is [price anchoring](https://en.wikipedia.org/wiki/Anchoring_effect). The highest price a seller will get for their car is often determined by the first price they mention. It's much better to allow the buyer to propose a price and then ask for more. By not listing a price or by listing unrealistic values like $9 million or $1, sellers can avoid anchoring their price.

With this in mind, I applied several filters to the dataset. I removed any cars priced under $2,000, as these were either scrap/parts cars or examples of price anchoring, and excluded any cars priced over $125,000. Most of the ads I found above $125,000 appeared illegitimate. Additionally, I removed any cars with over 300,000 miles, as this is a reasonable upper limit for a car's lifespan; most of these were likely examples obfuscating obscure poor condition. Lastly, I removed all duplicate rows, which eliminated about eleven percent of the data; this is do to people reposting ads to appear new and garner more clicks.

### 2. Feature Engineering
This process primarily consisted of two parts. The first part involved standard feature engineering. I created flags to indicate whether the ad included a description and whether it was a Carvana ad. Additionally, I converted the condition from a categorical to an ordinal variable. The categories "Excellent," "Fair," and "Poor" have a clear order, and this information should be retained. Similarly, I converted the number of cylinders from a string to a numeric type to preserve that ordinal information but this created a column of mixed types that polars would infrequently fail on; so this code has been removed and [an issue](https://github.com/AnAbominableSnowman/video_game_sales_predictions/issues/17) created to one day replace it.

The more interesting piece of feature engineering involved analyzing the description text. Much of the text analysis centers on classification and sentiment analysis. My initial approach was a bag-of-words style method, but I pivoted to[term frequency, inverse document frequency](https://builtin.com/articles/tf-idf). In short, words that appear in fewer descriptions are weighted more heavily (IDF), while words that appear frequently in a specific document are also weighted more heavily (TF). Ultimately, this results in a column of words along with their scores for each individual row (i.e., description). We can then use these as variables in our modeling later on.

## Modeling

| Model                             | Explanatory Variables            | Hyper Parameters                           | RMSE       | R²        |
|-----------------------------------|--------------------|------------------------------|------------|-----------|
| Ordinary Least Squares            | odometer       | `None`                                 | $12110     | 29.2%    |
| Log Least Squares                 | year, manufacturer, odometer, paint_color, state, title_status| `None`                                 | $10,986     | 41.8%    |
| LightGBM                          | all standard variables      | `{'learning_rate': 0.01, 'max_depth': 6,'boosting_type': "gbdt"` | $6180    | 81.7%   |
| LightGBM with HyperOpt and Text  | all standard variables, has_description?, is_carvana_ad?, 500 words (TF_IDF)|  `{'learning_rate': 0.05, 'max_depth': 8,'boosting_type': "gbdt",'number_of_leaves':166,'min_data_in_leaf': 5000}` | $4998 | 88%    |

*Table 1* The four modeling approaches


### 1. Simple Ordinary Least Squares
I iterated through four models in this project, progressively improving with each iteration. To assess model quality, I will use RMSE and R² while also favoring simpler models with equivalent success. My first model was the starting point for almost any good regression data science project: linear regression. My baseline was ordinary least squares regression with the odometer as my covariate.

The good news is that this model trains incredibly quickly and is the most interpretable of all models. I obtained a final RMSE of $12,000 and an R² of 29%. This means the model is off by about $12,000 on average, which isn't good enough for our use case. While the odometer explains about 29% of the total variation in price, the model's extreme interpretability is also a plus: any car's value starts at $31,140, and each consecutive mile reduces its value by 12.75 cents.

In the interest of brevity, I won't delve too deeply into checking the assumptions, as they all fail: linearity, homoscedasticity, and normality. While many types of inference are robust, this is too much for even the most robust linear regression inferences. Left with an uninterpretable and inaccurate model, it's time to go back to the drawing board. Another issue is the negative predictions; after 250,000 miles, the model starts predicting negative prices.

<div style="display: flex; gap: 40px;">
    <img src="results/Simple Linear Regression of Price by Odometer/residuals.png" alt="Image 1">
    <img src="results/Simple Linear Regression of Price by Odometer/ols_results.png" alt="Image 2">
</div>
_*Figure 1*_ Results of the simple least squares regression, note how many predictions are actually negative.  


### 2. Log Multiple Least Squares
In my next model, I aimed to tackle two issues: many negative price predictions and a reduction in bias at the cost of increased variance. To address the first problem, I started by logging the price. The intuition behind this approach is that price spans several magnitudes, and predicting the logarithm will never yield negative values.

The second strategy for addressing an underfitting model involved introducing additional terms: year, manufacturer, state, title status, and paint color. I selected these variables because they exhibit low cardinality, align with lived experience and common sense, and have low rates of missing values. Variables with a significant number of missing values would require extensive mean/mode imputation, which could diminish their value. Additionally, variables with high cardinality reduce model interpretability and risk fragmenting the data too thinly.

Reviewing the results below, the negative predictions have disappeared, and we observe an improvement in model accuracy, with approximately 40% of the variability in price explained by our model. Furthermore, we are now, on average, off by about $1,000 less, bringing the RMSE down to $11,000. While this is a great step toward accuracy, we now have 80 separate terms in the function. Additionally, we have correlated covariates skewing our beta coefficients, leading to peculiar results, such as cars in West Virginia being more expensive than those in New York.
<div style="display: flex; gap: 40px;">
    <img src="results/Log price Linear Regression/residuals.png" alt="Image 1">
    <img src="results/Log price Linear Regression/Log_mls_results.png" alt="Image 2">
</div>
_*Figure 12*_ Results of the log mulitiple least squares regression, note I cut off many rows in the results for brevity. Full results can be found in GH. 

### 3.Light GBM
Linear regression isn’t going to cut it on this dataset, so I'll try another tool: LightGBM. Let's begin with a naive approach, using all of our usable variables while leaving out any words, feature-engineered variables, or hyperparameter tuning. Consider this more of a baseline approach.

The great news is that this method provided a dramatic improvement in our model's performance. Our RMSE dropped down to $6,200, and our model can explain about 82% of the variation in price. This is a significant improvement in accuracy, but it did cost us a bit in terms of speed and explainability. Specifically, the time to fit the model increased from about 10 seconds for the previous model to around four minutes for this one. Given that this is a model that would likely be trained once a week at most, this trade-off is acceptable to me.

Addressing interpretability is a bit more challenging, but I will use [SHAP](https://shap.readthedocs.io/en/latest/index.html) to better understand how specific variables affect the predictions. The SHAP summary plot below can seem daunting at first. However, if we look at the first row, "year," we see that the farther right you go, the higher the SHAP value, which increases the predicted price. Most of the rightmost values are red, indicating high feature values, which aligns with our expectation: newer manufacturing years are likely to sell for more.

Odometer has a similar but opposite story for obvious reasons, more mileage, cheaper car. Interestingly, longitude, at the lowest values (light blue), is associated with positive increases in price (high SHAP values). This could be explained by the fact that the west coast and Hawaii—locations with lower longitude—are high-cost-of-living areas. An alternate explanation might be warmer climates out west use less salt in winter and therefore less rust perhaps.

Categorical variables are a bit harder to use in SHAP summary plots as they lack the clear sense of bigger to smaller and the resulting colorings. However, some categorical variables do tell a story. The far-right skew in the region seems to suggest region usually doesn't tell affect price much, but it has occasionally driven price up substanially. I haven't figured out how to pinpoint what values these are yet but  I suspect they correspond to these high-cost locations. Similarly, title status mostly has no effect on value, but certain titles can severely decrease the price. This aligns with our EDA, where most titles were clean, while some were salvage, parts, or lien, all of which can hinder a high sale price.

<div style="display: flex; gap: 40px;">
    <img src="results\light_gbm_basic/shap_summary_plot.png" alt="Image 1">
    <img src="results\light_gbm_basic\rmse_over_rounds.png" alt="Image 2">
</div>



### 4. Light GBM with HyperOpt and Words
From here, a kaleidoscope of options awaits: hyperparameter tuning, adding text data from the description, utilizing latitude and longitude to pull in zip code data, implementing cross-validation, and exploring the time and date of posting.

The first step was addressing overfitting, and while doing so, I set up HyperOpt. HyperOpt is one of my favorite tools for hyperparameter optimization. It uses a Bayesian framework to search more efficiently by using prior runs to inform future ones. An analogy: you are looking for a plane that flew between New York and London and crashed in the ocean. If you find debris, you should concentrate all your searchers in that spot to thoroughly comb the area.

I focused on optimizing three main hyperparameters: learning rate, maximum depth, the number of TF-IDF words, and L1 and L2 regularization. I did not tune the number of leaves, as I fixed that to be 65% of 2^max_depth. [Why?](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html).Because LightGBM grows leaf-first rather than level-first, fitting the maximum depth and setting the number of leaves to match it essentially mimics a level-first methodology similar to that of Random Forest. This approach is acceptable if overfitting is not an issue; however, since we are facing overfitting, we are opting for a different strategy.

The maximum number of TF-IDF features is set to 500, ie, the max, after several rounds of HyperOpt all pointed to that being optimal. Additionally, simultaneously applying both L1 and L2 regularization is inspired by ElasticNet, which combines the strengths of both regularization techniques to enhance model performance. This idea was inspired by [StackExchange](https://datascience.stackexchange.com/questions/57255/l1-l2-regularization-in-light-gbm).

Interpting TF_IDF, TO DO DEVIN



<div style="display: flex; gap: 40px;">
    <img src="results/light_gbm__hyperopt_and_feature_engineering/shap_summary_plot.png" alt="Image 1">
    <img src="results\light_gbm__HyperOpt_and_feature_engineering\rmse_over_rounds.png" alt="Image 2">
</div>

FIX UP ALT image titles


### Compare the given models.
Comparing the models, linear regression offers interpretability and statistical inference. However, given that none of the underlying assumptions hold and the presence of high correlation among features, its interpretative value diminishes. Therefore, LightGBM emerges as a more suitable model for this use case.

The next question is whether to use the model with TF-IDF and feature engineering or the base model. The more complex model (Model 4) takes about five times longer to train, but since it is intended to be trained only once a week, training speed is not a critical concern. Prediction speed is more important, as you will want to scrape data and generate predictions daily, if not hourly, to capitalize on deals promptly.

Ultimately, I envision using Model 4 alongside SHAP to create a waterfall plot that illustrates how the LightGBM model adjusts prices. This approach allows you to leverage subject matter expertise to validate, adjust, or disregard each prediction. Here’s an example from the model we trained:
TO DO: DEVIN, PLEASE FILL IN THE DETAILS

## Conclusion and Future Work
In conclusion, we determined that a feature-engineered model using LightGBM, which incorporates text data, is the best approach. We achieved an RMSE of XYZ and were able to explain approximately UFUF% of the variance in price. The most significant variables influencing predictions were odometer reading, region, year, and keywords XSSS, SSSD, KKKD, and DDDD. TO DO: DEVIN, PLEASE FILL IN THE DETAILS.

However, this approach has limitations. It is restricted to the data timeframe (early 2024), and we excluded about 15% of the total rows due to implausible odometer readings and prices. These corrupt data points were randomly distributed, making it difficult to assess their impact on the model and our interpretations.

Finally, while Carvana is a relatively minor player in the overall market, it is the largest within the fragmented Craigslist marketplace. Their pricing strategies could significantly influence market dynamics, and more attention should be directed toward their pricing practices.

## Future work
### 1. Renovate Craigslist Ad Descriptions
Pull out useful info from Craigslist Carvana ad's and delete the boiler plate. Today there is too much static data in the descriptions that overwhelems TF_IDF, so I turn Carvana ad's into a boolean varialbe. A further analysis could just delete the boiler plate and extract the nuggets of informations.


### 2. Download Images and build a CVML model
The dataset currently includes only image URLs. Although downloading all images and utilizing them is beyond the current project's scope, there is significant potential for future exploration. By implementing a computer vision machine learning (CVML) model, we could analyze these images to assess car quality, specifically looking for indicators such as rust, dents, and scratches. While I have experience in this area, I must keep the secret sauce secret.

